{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_entiry_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfmD5xJm_WAu",
        "outputId": "d40af1f4-06e0-4a62-bef1-0b374524bcc9"
      },
      "source": [
        "# Train NER from a blank spacy model\r\n",
        "import spacy\r\n",
        "\r\n",
        "nlp=spacy.blank(\"en\")\r\n",
        "\r\n",
        "nlp.add_pipe(nlp.create_pipe('ner'))\r\n",
        "\r\n",
        "nlp.begin_training()\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thinc.neural.optimizers.Optimizer at 0x7f5eb16e6c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z67f7UZg_hNx"
      },
      "source": [
        "# Import and load the spacy model\r\n",
        "import spacy\r\n",
        "nlp=spacy.load(\"en_core_web_sm\") \r\n",
        "\r\n",
        "# Getting the ner component\r\n",
        "ner=nlp.get_pipe('ner')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_p5prib_h0b"
      },
      "source": [
        "# New label to add\r\n",
        "LABEL = \"FOOD\"\r\n",
        "\r\n",
        "# Training examples in the required format\r\n",
        "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\r\n",
        "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\r\n",
        "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\r\n",
        "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\r\n",
        "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\r\n",
        "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\r\n",
        "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\r\n",
        "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\r\n",
        "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\r\n",
        "              (\"Chocolate souffl√© is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\r\n",
        "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\r\n",
        "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\r\n",
        "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\r\n",
        "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\r\n",
        "           ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceRQQDhv_wVt"
      },
      "source": [
        "# Add the new label to ner\r\n",
        "ner.add_label(LABEL)\r\n",
        "\r\n",
        "# Resume training\r\n",
        "optimizer = nlp.resume_training()\r\n",
        "move_names = list(ner.move_names)\r\n",
        "\r\n",
        "# List of pipes you want to train\r\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\r\n",
        "\r\n",
        "# List of pipes which should remain unaffected in training\r\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki_KN52U_0UN",
        "outputId": "abd5704c-8450-4b48-9693-e200de99c290"
      },
      "source": [
        "# Importing requirements\r\n",
        "from spacy.util import minibatch, compounding\r\n",
        "import random\r\n",
        "\r\n",
        "# Begin training by disabling other pipeline components\r\n",
        "with nlp.disable_pipes(*other_pipes) :\r\n",
        "\r\n",
        "  sizes = compounding(1.0, 4.0, 1.001)\r\n",
        "  # Training for 30 iterations     \r\n",
        "  for itn in range(30):\r\n",
        "    # shuffle examples before training\r\n",
        "    random.shuffle(TRAIN_DATA)\r\n",
        "    # batch up the examples using spaCy's minibatch\r\n",
        "    batches = minibatch(TRAIN_DATA, size=sizes)\r\n",
        "    # ictionary to store losses\r\n",
        "    losses = {}\r\n",
        "    for batch in batches:\r\n",
        "      texts, annotations = zip(*batch)\r\n",
        "      # Calling update() over the iteration\r\n",
        "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\r\n",
        "      print(\"Losses\", losses)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses {'ner': 4.069681805959251}\n",
            "Losses {'ner': 4.069684230851224}\n",
            "Losses {'ner': 4.156802422963758}\n",
            "Losses {'ner': 2.8984404841903597}\n",
            "Losses {'ner': 3.865231498328285}\n",
            "Losses {'ner': 6.205568944481701}\n",
            "Losses {'ner': 1.837203322105779}\n",
            "Losses {'ner': 2.8073690022918836}\n",
            "Losses {'ner': 2.807956418664686}\n",
            "Losses {'ner': 1.959394247453682}\n",
            "Losses {'ner': 3.927128174098913}\n",
            "Losses {'ner': 6.460989755912111}\n",
            "Losses {'ner': 1.1042752687627626}\n",
            "Losses {'ner': 1.1210485092076858}\n",
            "Losses {'ner': 3.611098411463143}\n",
            "Losses {'ner': 4.254951550121405}\n",
            "Losses {'ner': 8.28694293859553}\n",
            "Losses {'ner': 9.243206030787938}\n",
            "Losses {'ner': 2.013493734621866}\n",
            "Losses {'ner': 4.038300007165949}\n",
            "Losses {'ner': 4.038360251153705}\n",
            "Losses {'ner': 2.386202008826501}\n",
            "Losses {'ner': 6.581934008010952}\n",
            "Losses {'ner': 6.5856365860952675}\n",
            "Losses {'ner': 3.998152563511326e-05}\n",
            "Losses {'ner': 1.9953746470336604}\n",
            "Losses {'ner': 1.9953954245832968}\n",
            "Losses {'ner': 0.9484289026204493}\n",
            "Losses {'ner': 3.6999550551630023}\n",
            "Losses {'ner': 6.707702850925287}\n",
            "Losses {'ner': 3.3403675859735813}\n",
            "Losses {'ner': 4.4691064342914615}\n",
            "Losses {'ner': 5.505098076996388}\n",
            "Losses {'ner': 4.003734707832336}\n",
            "Losses {'ner': 5.940891356246322}\n",
            "Losses {'ner': 10.67416943706736}\n",
            "Losses {'ner': 2.8080918192863464}\n",
            "Losses {'ner': 10.043348610401154}\n",
            "Losses {'ner': 12.07456894090899}\n",
            "Losses {'ner': 2.727642675631728}\n",
            "Losses {'ner': 6.9370234755361935}\n",
            "Losses {'ner': 7.018637792013237}\n",
            "Losses {'ner': 1.2245000023394823}\n",
            "Losses {'ner': 1.224502122424525}\n",
            "Losses {'ner': 2.564298799465277}\n",
            "Losses {'ner': 1.7922105101320085e-05}\n",
            "Losses {'ner': 1.8622800067112024}\n",
            "Losses {'ner': 2.0021438516626944}\n",
            "Losses {'ner': 0.03905491768728098}\n",
            "Losses {'ner': 4.704574719724974}\n",
            "Losses {'ner': 7.153719247263615}\n",
            "Losses {'ner': 1.5833661350568313}\n",
            "Losses {'ner': 3.4803649787165467}\n",
            "Losses {'ner': 4.82562045525557}\n",
            "Losses {'ner': 3.758151240646839}\n",
            "Losses {'ner': 6.369405964274607}\n",
            "Losses {'ner': 7.704404733404319}\n",
            "Losses {'ner': 1.9350739844963414}\n",
            "Losses {'ner': 3.3556036646264715}\n",
            "Losses {'ner': 6.668260616013512}\n",
            "Losses {'ner': 3.609632481466619}\n",
            "Losses {'ner': 5.604472671272032}\n",
            "Losses {'ner': 9.33215845203851}\n",
            "Losses {'ner': 4.588301632098251}\n",
            "Losses {'ner': 8.667757095059642}\n",
            "Losses {'ner': 10.748627591226068}\n",
            "Losses {'ner': 5.020417928695679}\n",
            "Losses {'ner': 6.784495387693115}\n",
            "Losses {'ner': 8.414872934957716}\n",
            "Losses {'ner': 0.04976365556444762}\n",
            "Losses {'ner': 5.12302372445788}\n",
            "Losses {'ner': 7.554528815580397}\n",
            "Losses {'ner': 3.49170839227736}\n",
            "Losses {'ner': 7.179710681029963}\n",
            "Losses {'ner': 13.762614841052699}\n",
            "Losses {'ner': 1.3802717690142785}\n",
            "Losses {'ner': 2.5424809087249653}\n",
            "Losses {'ner': 2.664606287806802}\n",
            "Losses {'ner': 1.697720834859311}\n",
            "Losses {'ner': 1.7160321228662272}\n",
            "Losses {'ner': 6.679358762643961}\n",
            "Losses {'ner': 5.426649570465088}\n",
            "Losses {'ner': 6.444977414282732}\n",
            "Losses {'ner': 9.083398549316883}\n",
            "Losses {'ner': 2.8519071340560913}\n",
            "Losses {'ner': 2.8519079728093306}\n",
            "Losses {'ner': 5.293976054733493}\n",
            "Losses {'ner': 1.8182134265080094}\n",
            "Losses {'ner': 4.952665098801987}\n",
            "Losses {'ner': 6.20112816858304}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5aqlukJ_0ym",
        "outputId": "3265b954-67d1-41d1-dc57-88b6f1b429ec"
      },
      "source": [
        "# Testing the NER\r\n",
        "\r\n",
        "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\r\n",
        "doc = nlp(test_text)\r\n",
        "#print(\"Entities in '%s'\" % test_text)\r\n",
        "for ent in doc.ents:\r\n",
        "  print(ent)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities in 'manufacturing unit for cleaning chemicals'\n",
            "()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKn_sCmr_1PE",
        "outputId": "6b802f80-0d71-4f2d-8b01-1109011e7aab"
      },
      "source": [
        "# Output directory\r\n",
        "from pathlib import Path\r\n",
        "output_dir=Path('/content/')\r\n",
        "\r\n",
        "# Saving the model to the output directory\r\n",
        "if not output_dir.exists():\r\n",
        "  output_dir.mkdir()\r\n",
        "nlp.meta['name'] = 'my_ner'  # rename model\r\n",
        "nlp.to_disk(output_dir)\r\n",
        "print(\"Saved model to\", output_dir)\r\n",
        "\r\n",
        "# Loading the model from the directory\r\n",
        "print(\"Loading from\", output_dir)\r\n",
        "nlp2 = spacy.load(output_dir)\r\n",
        "assert nlp2.get_pipe(\"ner\").move_names == move_names\r\n",
        "doc2 = nlp2(' Dosa is an extremely famous south Indian dish')\r\n",
        "for ent in doc2.ents:\r\n",
        "  print(ent.label_, ent.text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to /content\n",
            "Loading from /content\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}